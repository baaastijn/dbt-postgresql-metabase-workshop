{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"DBT + PostgreSQL + Metabase = a first analytics platform","text":"<p>Step by step workshop to build a basic modern analytics platform with DBT, PostgreSQL and Metabase. Only open source tools, and can be hosted locally on your computer or in the cloud!</p> <p>I will assume this is the first time that you will use DBT and Metabase.</p> <p>Linked Github Repository, with fake data and exercise solutions.</p>"},{"location":"#hello-what-are-we-going-to-do-tonight-brain","title":"Hello!  What are we going to do tonight, Brain?","text":""},{"location":"#discovering-awesome-open-source-data-tools","title":"Discovering awesome open source data tools","text":"<p>Initial goal is to have fun discovering some of most notorious modern data tools, with a typical use-case: analyze your orders and customers.</p> <p>The BI platform pipeline that we will create is similar to this schema, whith dashboards as outputs: </p> <p></p> <p>Over the years, DBT has trusted the charts as the \"transformation workflow\" tool, same for Metabase as open source business intelligence tool, along with Apache Superset. They work pretty well together, and are seen in many production architectures.</p> <p>The last piece of data software that we will discover is PostgreSQL, who will act as a datawarehouse. While Clickhouse, Google BigQuery or Snowflake might have been excellent choices for an analytical production platform, I selected PostgreSQL because it can be installed locally with zero expenses, and is a perfect fit for our light and standard needs.</p>"},{"location":"#building-an-analytics-platform","title":"Building an analytics platform","text":"<p>Over this workshop, you will get a first introduction to DBT and Metabase features.</p> <p>Our use case is quite simple: we are a company, with customers, orders and payments. We want to build nice visualization dashboards, to monitor closely our activity.</p> <p>More than dashboards, we want to avoid black magic, avoid technical debt, and for that we will use DBT to introduce best practive for analytics.</p> <p>What this workshop is not : it's not an intensive worshop for experts, neither something to follow for best practices (it's more the opposite since we will skip few phases like Git synchonization, DBT, tests).</p>"},{"location":"#what-we-are-not-building-today","title":"What we are not building today","text":"<p>This is not a workshop to craft a modern data platform ready for production. This is not an installation guide, best-practices neither an extensive documentation.</p>"},{"location":"#time-and-cost","title":"Time and cost","text":"<p>Expected time for this workshop : it depends of your skills. If you are new to the cloud, let's say 4-5 hours. If your have cloud/tech experience, +-2 hours.</p> <p>Cost : free if you run everything locally. </p> <p>In the cloud, more or less 3 euros for 24h in OVHcloud:</p> <ul> <li>AI Notebook (DBT host): 24h x 0,03 \u20ac = 0,72\u20ac</li> <li>PostgreSQL managed database: 24h x 0,07\u20ac = 1,68\u20ac</li> <li>VM instance (metabase host): 24h x 0,0088\u20ac = 0,021\u20ac</li> </ul> <p>Ready? See you in few hours! Start by setting up your environment</p>"},{"location":"part1setup/","title":"Part 1 -  Setup your environment","text":""},{"location":"part1setup/#cloud-or-not-cloud-chose-wisely","title":"Cloud or not cloud? Chose wisely!","text":"<p>This workshop will require to install few things such as DBT (Python package), a PostgreSQL client and Metabase (Docker image for the easiest installation way).</p> <p>You can opt to install everything locally on your computer if you have enough access rights to do it.</p> <p>You can also prefer to use cloud products such as virtual machines or Python Notebook.</p> <p>The main advantage for this method is the ability to start with a clean development environment, and be able to delete safely everything at the end. Also, if you don't have admin on your own computer, it will be easier. Last but not least, you can share easily your cloud environment with someone for troubleshooting. Usefull during a workshop!</p> <p>On the dark side, it can be seen as a bit complex if your are new with cloud. So, chose wisely!</p>"},{"location":"part1setup/#requirements","title":"Requirements","text":"<p>Requirements may differ if you replay this workshop in the cloud or on your computer.</p>"},{"location":"part1setup/#if-running-on-your-computer","title":"If running on your computer","text":"<p>Requirements:</p> <ul> <li>A Python &gt;= 3.8 environment with ability to install new packages such as DBT</li> <li>A PostgreSQL database (we will see how to install it)</li> <li>A PostgreSQL client to query a PostgreSQL database (like psql or PgAdmin)</li> <li>Ability to run Docker images (locally or in the cloud) or execute JAR file (for Metabase installation)</li> <li>basic technical skills such as building a SQL query, connect with SSH, ...</li> </ul>"},{"location":"part1setup/#if-running-in-the-cloud","title":"If running in the cloud","text":"<p>You will need the same thing, but with... managed cloud products.</p> <p>For the next step, we will go for cloud products with OVHcloud, a european cloud provider. From an architecture perspective, it will look like this in the cloud:</p> <p></p> <p>Requirements:</p> <ul> <li>OVHcloud account created</li> <li>Python environment : OVHcloud AI Notebook (AWS doppeldanger : AWS Sagemaker notebook)</li> <li>Database: OVHcloud PostgreSQL (AWS: RDS for PostgreSQL)</li> <li>DBT: installed in a OVHcloud Jypter Notebook (AWS: Sagemaker Notebook)</li> <li>Metabase: installed in a OVHcloud virtual Machine (AWS: EC2 VM)</li> <li>PostgreSQL client: PgAdmin</li> <li>SSH client (Windows: Putty or similar)</li> <li>SSH key configured (to access virtual machines remotely)</li> </ul>"},{"location":"part1setup/#step-1-optional-prepare-a-cloud-python-environment","title":"Step 1: (optional) prepare a cloud Python environment","text":"<p> As explained previously, we will use OVHcloud products in this workshop. It's optional, you can skip this step if you prefer to use your own Python environment (your computer for example).</p> <p>This workshop needs a Python environment, with the ability to browse files easily. To be more visual and practical, we will opt for a Python notebook code editor. Most used ones in the world are JupyterLab and VSCode. We will create one in the cloud now.</p> <p>If required, create an OVHcloud free account or log in: </p> <ol> <li>Log in OVHcloud control panel: https://www.ovhcloud.com.</li> <li>Go to Public Cloud section in the top menu.</li> <li>Create a new project if required.</li> </ol> <p>Once your project is created, create a new AI Notebook with these parameters:</p> <ul> <li>name: dbt-notebook  </li> <li>code editor: Jupyter lab</li> <li>Framework: Miniconda with python &gt;=3.8</li> <li>Privacy: public access (anyone will have access to it. useful for a workshop, NOT recommended for production or sensitive information)</li> <li>Datacenter: as you wish</li> <li>Resource: CPU x1</li> <li>Attach a git repo or data container: no</li> <li>SSH key: no</li> </ul> <p>Once your notebook is running (it should take less than 1 minute), you can access it by clicking on <code>JupyterLab</code> button:</p> <p></p> <p>This notebook is a Linux environment running inside a Docker image. It gives you the ability to live code directly in your web browser. You can install additional package swith classic Python commands such as <code>pip install</code>and <code>conda install</code>.  Also, you can share your environment with someone else just by sharing your notebook URL. Very useful during a workshop session when you need some help .</p> <p>Now, click on the <code>Terminal</code> icon inside this notebook. </p>"},{"location":"part1setup/#step-2-check-your-python-environment","title":"Step 2: check your Python environment","text":"<p>This tutorial requires Python &gt;= 3.8. To check your current Python environment, type in a terminal:</p> <pre><code>$ python --version\nPython 3.9.5\n</code></pre> <p>Mine is on version 3.9.5. If you have a deprecated version, please upgrade it at least to &gt;=3.8.</p> <p> A best practice is to create a new python environment with Conda. In this workshop, we will not since we are creating a new linux environment that we will trash just after. But feel free to do it, especially with local installation. If your a new with that, look for Conda environments in your preferred search engine.</p>"},{"location":"part1setup/#step-3-set-up-a-postgresql-server","title":"Step 3: set up a PostgreSQL server","text":"<p>For this workshop, we will use a PostreSQL server to store and transform our data.</p> <p>PostgreSQL is an open source and community-based transactional database management system, widely used accross the world. Perfect for a workshop but also for production. It has not the flexibily of modern lakehouse such as Snowflake or BigQuery (compute and storage are linked for example, and information is stored in row format, not columnar) but can be relevant in many Business Intelligence use-cases where performance is not the main criteria.</p> <p>To setup a new database server, two solutions:</p> <ul> <li>Use a managed Cloud Product such as OVHcloud for PostreSQL (paid).</li> <li>install yourself a PostgreSQL somewhere (eg. your computer, free).</li> </ul>"},{"location":"part1setup/#option-1-launch-an-ovhcloud-managed-postgresql","title":"Option 1: launch an OVHcloud managed PostgreSQL","text":"<p>We will go for a OVHcloud product here:</p> <ol> <li>Go to OVHcloud control panel.</li> <li>Go to Public Cloud section in the top menu.</li> <li>Create a new project or select an existing one.</li> </ol> <p>Once your project is ready, select <code>Databases</code>in the left menu then create a new one with these parameters:</p> <ul> <li>Database type: PostgreSQL 14</li> <li>Plan: Essential (1 node)</li> <li>Region: as you wish</li> <li>Size of nodes: smallest one, like DB1-4</li> <li>Options / Network: Public network (open to internet access)</li> </ul> <p>Launch this managed database server. Once this server is up and running, you will have to configure users and authorized IPs.</p> <p>Go in <code>Users</code> tab, and regenerate a password for user <code>**avdadmin**</code>. Copy his password safely. You can also opt for a new user creation via a PostgreSQL client if you are more comfortable. Be sure to grant him admin role.</p> <p>Now go in <code>Authorized IPs</code> and add the IP 0.0.0.0/0. It's a wildcard allowing any IP in the world. </p> <p> an IP Wildcard is useful for a workshop or troubleshooting, but not recommended at all for production. Anyone will be able to contact your cluster.</p>"},{"location":"part1setup/#option-2-manual-installation-self-hosted","title":"Option 2: manual installation (self-hosted)","text":"<p>If you opt for a manual installation, follow official instructions here: https://www.postgresql.org/download/. This workshop was tested with PostgreSQL 14.</p> <p>Once installed, configure a new database.  You can find alternative websites with detailled tutorials like https://www.postgresqltutorial.com/postgresql-getting-started/ to guide you through the steps.</p> <p> installing a database server, transformation tools and dataviz tool in the same local environment is really dangerous. These stunts are performed by trained professionals, don't try this in production..</p>"},{"location":"part1setup/#setup-done","title":"Setup done!","text":"<p>The full workshop environment is now up and running! Now move to the Part 2: install DBT</p>"},{"location":"part2dbt/","title":"Part 2 - Install DBT","text":"<p>Great! On the previous part, we setup a Python environment and a datawarehouse via PostgreSQL. It's now time to install the first data software, DBT!</p>"},{"location":"part2dbt/#what-the-hell-is-dbt","title":"What the hell is DBT?","text":"<p>Good question! I still don't know exactly after few hours. DBT is the acronym of Data Build Tool and was created in 2016 by some folks of RJMetrics.</p> <p>Here is quote from their official website: dbt is a transformation workflow that helps you get more work done while producing higher quality results. You can use dbt to modularize and centralize your analytics code, while also providing your data team with guardrails typically found in software engineering workflows. Collaborate on data models, version them, and test and document your queries before safely deploying them to production, with monitoring and visibility.</p> <p>DBT is an answer to all team facing analytics issues. Who made a change on this query?. Where can I find explanation about this data? How can i rollback to the queries made 1 week ago? How can I test safely before running is in production?. You see what I mean. Artisanal mode.</p> <p>If I try to sum-up, with data analytics your start with multiple data sources and end up with results, like reports or dashboards. Between sources and results, the workflows you put has to be structured, tested, documented, collaborative, and shared safely.</p> <p>We already do that widely for software. A devops culture. If you develop a software, you will push you code on Git, collaborate with branches, do some versioning, document you code directly, build CI/CD pipelines, ... </p> <p>Why not doing the same with data analytics ? Software engineering is here since long, now let's embrace analytics engineering! </p> <p>DBT is one of the answer :) Don't hesitate to read more with the DBT viewpoint.</p>"},{"location":"part2dbt/#install-dbt","title":"Install DBT","text":"<p> Check official documentation for complete guidance: https://docs.getdbt.com/docs/get-started/installation.</p> <p>DBT comes in two versions:</p> <ul> <li>DBT Core, that you can install yourself as self-hosted. It's free and open source.</li> <li>DBT Cloud, where you will get a managed version of DBT, with more features. It's a paid plan.</li> </ul> <p>We will install DBT Core in our case.</p> <p>Since we will use DBT with PostgreSQL, we will install DBT Core and DBT PostgreSQL connector at once.</p> <p>Go in you Python terminal and type:</p> <pre><code># Update PIP to the latest version\n$ python -m pip install --upgrade pip\n(...)\n# Install DBT Core and DBT PostgreSQL connector at once\n$ pip install dbt-postgres\n(...)\n# Verify versions\n$ dbt --version\nCore:\n  - installed: 1.3.1\n  - latest:    1.3.1 - Up to date!\n\nPlugins:\n  - postgres: 1.3.1 - Up to date!\n</code></pre>"},{"location":"part2dbt/#create-your-first-project","title":"Create your first project","text":"<p>DBT works with projects, containing your configuration, SQL models, and much more.</p> <p>Create your first one with <code>DBT init</code>:</p> <pre><code>$ dbt init quick_workshop\n09:47:12  Running with dbt=1.3.1\n09:47:12  Creating dbt configuration folder at /workspace/.dbt\nWhich database would you like to use?\n[1] postgres\n(...)\nYour new dbt project \"quick_workshop\" was created!\n</code></pre> <p>2 new folders appeared in your directory, <code>Logs</code>and <code>quick_workshop</code>.</p> <p>Move to this project folder with your terminal:</p> <pre><code># Go to new project folder\n$ cd quick_workshop\n\n# list content files\n$ ls\nREADME.md  analyses  dbt_project.yml  macros  models  seeds  snapshots  tests\n</code></pre> <p>Congrats, your first project is created!</p> <p>Quick description about these directories:</p> Directory Description analyses where you can compile SQL queries, more often for later usage as analytical queries macros blocks of code that you can reause multiple times models where you put your code. 1 file = 1 model, and you code quite often transform raw data in datasets or intermediate trandsformations seeds Static CSV data that you can load via DBT snapshots when you capture the state of your data tables, to refer to it later tests SQL/Python tests you can run to validate your data or models. <p>When you initialize a DBT project, there is also the file <code>dbt-project.yml</code>, which contains useful parameters. A name, a version, models to build but also a profile to use.</p> <p>As explained before, DBT does not process data itself. There is no compute, no \"power\". DBT is linked to something doing transformation tasks. Most famous ones are PostreSQL, BigQuery, Snowflake, Spark, ...  Here in profile, you can redirect this project to a profile to use.</p> <p>Keep it like the screenshot ie. <code>quick_workshop</code>.</p> <p></p>"},{"location":"part2dbt/#connect-dbt-to-your-postgresql-server","title":"Connect DBT to your PostgreSQL server","text":"<p>DBT connects to your datawarehouse using a profile, which is a <code>.yml</code> file created during our first project init. You were notified about his creation during the <code>dbt init</code>in the previous step, and his directoy path was also shown.</p> <p>Let's edit this file:</p> <ol> <li>Open your Python terminal.</li> <li>Open your <code>profile.yml</code> file, in our case it's in <code>/workspace/.dbt/profiles.yml</code>.</li> </ol> <pre><code># Open profile.yml with your preferred code editor such as Vim/nano/...\n$ nano /workspace/.dbt/profiles.yml\n</code></pre> <p>By default, your profile should look like this:</p> <pre><code>quick_workshop:\n  outputs:\n\n    dev:\n      type: postgres\n      threads: [1 or more]\n      host: [host]\n      port: [port]\n      user: [dev_username]\n      pass: [dev_password]\n      dbname: [dbname]\n      schema: [dev_schema]\n\n    prod:\n      type: postgres\n      threads: [1 or more]\n      host: [host]\n      port: [port]\n      user: [prod_username]\n      pass: [prod_password]\n      dbname: [dbname]\n      schema: [prod_schema]\n\n  target: dev\n</code></pre> <p>Replace values with your PostgreSQL server informations.</p> <p>In our case, with an OVHcloud PostgreSQL service:</p> <pre><code>quick_workshop:\n  outputs:\n\n    dev:\n      type: postgres\n      threads: 4\n      host: postgresql-5c66e728-o90e8df85.database.cloud.ovh.net\n      port: 20184\n      user: avdadmin\n      pass: hmTiDdN0y*********\n      dbname: defaultdb\n      schema: development\n      sslmode: require\n\n    prod:\n      type: postgres\n      threads: 4\n      host: postgresql-5c66e728-o90e8df85.database.cloud.ovh.net\n      port: 20184\n      user: avnadmin\n      pass: hmTiDdN0y*********                                \n      dbname: defaultdb\n      schema: production\n      sslmode: require\n\n  target: dev\n</code></pre> <p>Notice few things :</p> <ul> <li>You can get a development and a production environment, or even more.  Here, in the target, we ONLY interact with dev.</li> <li>You can differenciate the schemas used if you want to. We did it there, with the same database server BUT two schemas.</li> <li>SSL mode is required for OVHcloud databases services, but not if you are running PostgreSQL locally.</li> <li>1 thread means no tasks parralelization. Default is 4. it mean DBT will run 4 jobs in parralel. If you put it to 1, it will wait to end the first tasks to start a new one.</li> </ul> <p>A best practice is to fully separate development and production environnment. First to avoid human mistakes such as data deletion, but also to isolate compute resources. Having an splitted dev platform will allow you to run intensive queries without being scared to \"disturb\" production performances. </p> <p>Save this configuration and close this <code>profile.yml</code> file.</p>"},{"location":"part2dbt/#test-your-environment","title":"Test your environment","text":"<p>First, run a debug:</p> <pre><code>quick_workshop$ dbt debug\n</code></pre> <p></p> <p>If all checks have passed, we are good! DBT is able to find your configuration and able to connect to PostgreSQL.</p>"},{"location":"part2dbt/#perform-a-first-dummy-dbt-run","title":"Perform a first dummy DBT run","text":"<p>During the project initialization, DBT pushed examples inside the <code>models</code> folder.</p> <p>When your perform a DBT run, DBT looks for models inside this folder and will run them.</p> <p>If you go back to the previous step, you will notice at the end of your <code>dbt_project.yml</code> configuration that we asked to build models inside /models/examples.</p> <p>Perform your first run:</p> <pre><code>quick_workshop$ dbt run\n(...)\n</code></pre> <p></p> <p>As shown in the result, 2 models were completed successfully.</p> <p>These models are dummy ones. You can check what's inside by browing into <code>/models/examples</code> and open the <code>.SQL</code> files.  In short, the first SQL model will perform a SELECT on a fake source data, and the second DBT model will perform a SELECT on top of the first SQL model. </p> <p>The good thing is, DBT is able to materialize results. so you can reuse your results easily (like, hmmm, for BI reports maybe ?). It was the case for these two models. </p> <p>The most used materializations are <code>views</code>and second ones are <code>tables</code>. A <code>view</code> can be seen as a virtual table. every time you ask for it, the model is rebuilt. It does not store data in your datawarehouse but will virtually aggregate information to create something to view. A <code>table</code> will create a real table in your datawarehouse. You wrote something on disks. </p> <p> More information and more materialization options are available with DBT https://docs.getdbt.com/docs/build/materializations.</p> <p>Both have pros and cons, and the power of DBT is that you can specify this materialization directly in your models. </p> <p>As an example, if I select <code>table</code> here is what i can see inside my PostgreSQL cluster now:</p> <p></p>"},{"location":"part2dbt/#dbt-is-correctly-initialized","title":"DBT is correctly initialized!","text":"<p>Now that we runned our first DBT run, let's bring datasets and build our own models! </p> <p>Go to the next part, data ingestion.</p>"},{"location":"part3ingest/","title":"Part 3 - Ingest data (badly)","text":"<p>Your DBT project is now ready to be used. Time to play with data!</p>"},{"location":"part3ingest/#about-dbt-and-seeds","title":"About DBT and seeds","text":"<p>DBT allows CSV data ingestion, called <code>seeds</code> (remember the seeds folder?).</p> <p>Best practive is to use <code>seeds</code> when you need to control static and versionned data.</p> <p>Imagine that you are running everytime the same analysis for your business, but you want to exclude a list of 20 internal accounts. How ? Well, by using seeds.</p> <p>Seeds workflow is:</p> <ol> <li>add CSV files into your quick-workshop/seeds/ folder.</li> <li>run the <code>dbt seed</code> command</li> <li>CSV files are loaded as tables inside your datawarehouse.</li> <li>if you modify your CSV by adding lines, the tables will be updated during the need <code>dbt seed</code>.</li> </ol> <p> More information and more options: https://docs.getdbt.com/docs/build/seeds. For example you can modify the <code>seeds</code>default directory, or specify a column datatype instead of betting on autoselection.</p>"},{"location":"part3ingest/#download-fake-data","title":"Download fake data","text":"<p> For easiness, this fake data is inspired from official dbt example called <code>jaffle_shop</code>but with extra columns for better use-cases with Metabase (more fields, and more lines). Full credits goes to this official DBT example repository.</p> <p>Download the fake data provided in this workshop Github repository. It consist of 3 CSV files, wike fake data (generated with www.Mockaroo.com).</p> <p>Copy these CSV files inside the <code>quick_workshop/seeds</code> folder.</p> <p>You can do if with click-cloick-click in your Jupyter Notebook, or just get them via your Python terminal:</p> <pre><code># Move to the Seeds folder\nquick_worksop$cd seeds\n\n# Download 3 CSV file from this Github repository\nseeds$wget https://raw.githubusercontent.com/baaastijn/dbt-postgresql-metabase-workshop/main/fake_data/raw_orders.csv\nseeds$wget https://raw.githubusercontent.com/baaastijn/dbt-postgresql-metabase-workshop/main/fake_data/raw_customers.csv\nseeds$wget https://raw.githubusercontent.com/baaastijn/dbt-postgresql-metabase-workshop/main/fake_data/raw_payments.csv\n\n# Go back to your main DBT directory\nseeds$ cd ..\n</code></pre>"},{"location":"part3ingest/#ingest-fake-data","title":"Ingest fake data","text":"<p>For this workshop, we will tweak the use of <code>seed</code> to import fake data.</p> <p> Usually you don't need to bring fake data like this, because you already have something in yur database. But hey it's a workshop . If you want to extend this workshop, I recommend to ingest data with open source tools such as Airbyte or Meltano. Both can also be self-hosted or provided as SaaS offers.</p> <p>From a entity relationship diagram (ERD) point of view, data is linked like this:</p> <p></p> <p>Now that your data is downloaded inside your <code>quick_workshop/seeds</code> directory, run a <code>DBT seed</code>:</p> <pre><code>quick_workshop$dbt seed --full-refresh\n14:38:29  Running with dbt=1.3.1\n14:38:29  Found 2 models, 4 tests, 0 snapshots, 0 analyses, 289 macros, 0 operations, 3 seed files, 0 sources, 0 exposures, 0 metrics\n14:38:29  \n14:38:30  Concurrency: 4 threads (target='dev')\n14:38:30  \n14:38:30  1 of 3 START seed file development.raw_customers ............................... [RUN]\n14:38:30  1 of 3 OK loaded seed file development.raw_customers ........................... [INSERT 100 in 0.31s]\n14:38:30  2 of 3 START seed file development.raw_orders .................................. [RUN]\n14:38:32  2 of 3 OK loaded seed file development.raw_orders .............................. [INSERT 1000 in 1.72s]\n14:38:32  3 of 3 START seed file development.raw_payments ................................ [RUN]\n14:38:33  3 of 3 OK loaded seed file development.raw_payments ............................ [INSERT 1000 in 1.36s]\n14:38:33  \n14:38:33  Finished running 3 seeds in 0 hours 0 minutes and 3.66 seconds (3.66s).\n14:38:33  \n14:38:33  Completed successfully\n14:38:33  \n14:38:33  Done. PASS=3 WARN=0 ERROR=0 SKIP=0 TOTAL=3\n</code></pre> <p>Data is now imported! Pushed as 3 x tables into your datawarehouse.</p> <p>when taking a look with a PostgreSQL client like PgAdmin, you can see three new tables :</p> <p></p>"},{"location":"part3ingest/#data-is-now-loaded","title":"Data is now loaded!","text":"<p>Data is loaded, it's now time to generate analytics-ready views and tables! Go to the next part, with transformations.</p>"},{"location":"part4transform/","title":"Part 4 - Transform data with DBT","text":"<p>The current data is exhaustive but splitted in multiple tables.</p> <p>You have the users, the orders, the payments:</p> <p></p> <p>How can you get more actionable views? For example:</p> <ul> <li>For each customer, a total amount of orders.</li> <li>The last order from each customer.</li> <li>Revenues per country.</li> <li>...</li> </ul> <p>Without DBT, you can get those answers with classic SQL syntax, directly by querying PostgreSQL  It will work fine. But wait.</p> <p>Imagine that you don't have one query but one of a dozen, nested together like pipelies (first anonymize data, then remove fraud, then calculate revenues, ...) not run punctually but with recurrence, with vast amount of data, a critical production. Also imagine that you want to share your queries globally and with control, with versionning, tests, generated documentation and co? </p> <p>That's when DBT is relevant, bringing your transformation workflows and control over SQL queries or Python code.</p>"},{"location":"part4transform/#create-your-first-sql-model","title":"Create your first SQL model","text":"<p>Browse your <code>quick_workshop/models/</code> directory.</p> <p>Delete the <code>example</code> folder, not required anymore.</p> <p>Create a new file named <code>customers.sql</code>. Open this file and copy the code below:</p> <pre><code>with customers as (\nselect\nid as customer_id,\nfirst_name,\nlast_name,\nemail,\ncountry\nfrom {{ ref('raw_customers') }}\n),\norders as (\nselect\nid as order_id,\ncustomer_id,\norder_date,\norder_item,\norder_status\nfrom {{ ref('raw_orders') }}\n),\ncustomer_orders as (\nselect\ncustomer_id,\nmin(order_date) as first_order_date,\nmax(order_date) as most_recent_order_date,\ncount(order_id) as number_of_orders\nfrom orders\ngroup by 1\n),\nfinal as (\nselect\ncustomers.customer_id,\ncustomers.first_name,\ncustomers.last_name,\ncustomers.country,\ncustomer_orders.first_order_date,\ncustomer_orders.most_recent_order_date,\ncoalesce(customer_orders.number_of_orders, 0) as number_of_orders\nfrom customers\nleft join customer_orders using (customer_id)\n)\nselect * from final\n</code></pre> <p>Save this file.</p> <p>Few explanations about this code sample:</p> <ul> <li>We start by selecting few columns for the Table <code>raw_customers</code>.</li> <li>We do the same from the table <code>raw_orders</code>.</li> <li>We then create new columns, respectively the first order date, the most recent, and the total or orders.</li> <li>we build a final query regrouping columns from multiple parts.</li> <li>finally we select everything (*) from this query.</li> <li>SQL info: the coalesce() function give you the first <code>NOT NULL</code> result. Here if <code>number_of_orders</code>is null, it will replace it by zero to avoid empty cells.</li> </ul>"},{"location":"part4transform/#reconfigure-your-project","title":"Reconfigure your project","text":"<p>During DBT project initialization, DBT was configured to run models only from <code>/examples</code> directory.</p> <p>Current configuration inside <code>dbt_project.yml</code> is:</p> <pre><code>models:\n    quick_workshop:\n        examples\n            +materialized: table\n</code></pre> <p>Since we deleted the <code>quick_workshop/models/examples</code> directory, we have to modify this part.</p> <p>Modify the file <code>dbt_project.yml</code>and put this new configuration instead:</p> <pre><code>models:\n    quick_workshop:\n        materialized: table\n</code></pre> <p>It will now take into account our SQL files pushed at the root of <code>quick_workshop/models</code>.</p>"},{"location":"part4transform/#run-dbt","title":"Run DBT","text":"<p>Now let's run DBT. It will browse the quick_workshop/models directory:</p> <pre><code>quick_workshop$ dbt run\n10:24:53  Running with dbt=1.3.1\n10:24:53  Found 1 model, 0 tests, 0 snapshots, 0 analyses, 289 macros, 0 operations, 3 seed files, 0 sources, 0 exposures, 0 metrics\n10:24:53  \n10:24:53  Concurrency: 4 threads (target='dev')\n10:24:53  \n10:24:54  1 of 2 START sql table model development.customers ............................. [RUN]\n10:24:54  1 of 2 OK created sql table model development.customers ........................ [SELECT 100 in 0.11s]\n10:24:54  \n10:24:54  Finished running 1 table model in 0 hours 0 minutes and 0.56 seconds (0.11s).\n10:24:54  \n10:24:54  Completed successfully\n10:24:54  \n10:24:54  Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1\n</code></pre> <p>Model was run successfully and table also created.</p>"},{"location":"part4transform/#check-the-result-in-your-datawarehouse","title":"Check the result in your datawarehouse","text":"<p>What we are waiting for, is a new table created in our datawarehouse, countaining our aggregated data.</p> <p>Good news this is exactly what we have .</p> <p></p>"},{"location":"part4transform/#exercise-1-build-another-dbt-model","title":"Exercise 1: build another DBT model","text":"<p>Now that we discovered a bit more how DBT works, let's try to build another model yourself.</p> <p>Our Marketing team would love to target more accurately some geographical areas, and want to create a country dashboard. </p> <p>You have to create a new table in our datawarehouse, analyzing revenues per country. Also, we only want to summarize revenues for orders where status is true (meaning it's paid).</p> <p>The table columns should be like this:</p> Country Total of orders Total revenue France 37 2530 ... ... ... <p>Create this new table and save it inside quick_workshop/models/countries.sql.</p> <p> Hint: required data is splitted into three tables (customers, orders, payment). Time to learn about SQL JOIN .</p> <p>If you are blocked, you can find the solution in this Github repository, in the exercises folder.</p>"},{"location":"part4transform/#exercise-2-remove-suspicious-accounts","title":"Exercise 2: remove suspicious accounts","text":"<p>Our Data team is convinced that we need to remove all emails ending with @facebook.com, because 100% of the time it's fraudulous accounts.</p> <p>The goal here is to skip some irrelevant data that are ingested regularly. They also told us that they are investigating, but other emails system will have to be banned in the future.</p> <p>How do you proceed ?</p> <p>Same, if you are blocked, a solution can be found in this Github repository!</p>"},{"location":"part4transform/#generate-documentation","title":"Generate documentation","text":"<p>Let's get back to the topic of building a first BI Platform.</p> <p>What the point of having data tables without a clean documentation? None. Over time, a good documentation is key. Code commenting is a best practice in software development, data and queries documentation should also become an acceptence criteria for analytics engineering.</p> <p>What if we could generate documentation on the fly and serve it to everyone, for, by example, spread it to the data team?</p> <p>Good news it's included in DBT! It provide few things: - Ability to document your models directly in your repository (inside your models files, but also in separated documents) - Ability to generate a clean HTML documentation - Ability to serve this documentation on a web server.</p> <p> Inception: More documentation about documentation: https://docs.getdbt.com/docs/collaborate/documentation</p> <p>Go back to your Python terminal and type:</p> <pre><code>$ dbt docs generate\n(...)\n$ dbt docs serve --port 8001\n</code></pre> <p>Your documentation is now generated as HTML, and hosted!</p> <p>Generated files are pushed inside quick_workshop/target.</p> <p>If you are running DBT locally, go to http://localhost:8001.</p> <p>If you are using OVHcloud AI Notebook, it support port forwarding. Take the link of you notebook and add the port in the URL as shown below :</p> <p>URL example : https://f746650a-e99c-4ab5-b7e4-5d79c32cb134.notebook.gra.ai.cloud.ovh.net/lab/tree/quick_workshop</p> <p>Access port 8001: https://f746650a-e99c-4ab5-b7e4-5d79c32cb134-8001.notebook.gra.ai.cloud.ovh.net</p> <p></p>"},{"location":"part4transform/#thats-a-one-small-step-for-man-but","title":"That's a one small step for man, but...","text":"<p>Congrats, you now had a first model running, and hopefully a good understanding about few DBT features.</p> <p>Lot of shortcuts were made here.</p> <p>Typical DBT workflows includes more than one source of data, quite often dozens of models, creating views, tables, and all of them are nested together. With DBT test, and DBT macros, CI/CD, and Git synchronization.</p> <p>Did i forgot to mention it? Use Git for your files . </p> <p>Again, you can see it as software development best practices, but ported to analytics code. No more no less!</p> <p>The DBT part is over for this workshop. The last part will engage data visualization!</p>"},{"location":"part5metabase/","title":"Part 5 - Visualize with Metabase","text":""},{"location":"part5metabase/#why-is-metabase-so-cool","title":"Why is Metabase so cool?","text":"<p>Similarly to DBT, Metabase is an open source software provided as a paid cloud version or free self-hosted.</p> <p>Metabase is awesome in multiple ways. It can be defined as a BI tools, with two main parts:</p> <ul> <li>A query tool, with wysiwyg editor and SQL query support.</li> <li>A dashboard tool, with a well polished and intuitive interface.</li> </ul> <p>Metabase does not store your data, but they will store your queries syntax and metadata required to build your dashboards. I guess that the name meta(data)base comes from this point . By default, for the self-hosted version, they store this data inside a SQLite database but you can opt for your own SQL database as a backend (much better for production). </p>"},{"location":"part5metabase/#install-metabase","title":"Install Metabase","text":"<p>For this tutorial, we will use the self-hosted free version.</p> <p>Metabase can be installed from multiples ways, as explained in their official documentation: java JAR file, Docker image, or from source.  </p>"},{"location":"part5metabase/#option-1-install-metabase-with-desktop-jar-version","title":"Option 1: install Metabase with desktop JAR version","text":"<p>Metabase is proposed a a Java file (JAR). If you computer has a Java Runtime &gt;=8 (JRE), such as MacOS or most of Linux/Unix distributions, it's the easiest way to try it.</p> <p>Express guide : </p> <ol> <li>download Metabase from https://www.metabase.com/docs/latest/installation-and-operation/installing-metabase</li> <li>push it in a folder called for example \"metabase\". because it will generate files.</li> <li>Open a terminal, go to this folder and type <code>java -jar metabase.jar</code>.</li> </ol>"},{"location":"part5metabase/#option-2-install-metabase-via-docker-in-the-cloud","title":"Option 2: Install Metabase via Docker in the cloud","text":"<p>If you are more into Docker or source installation, follow their official tutorials on  https://www.metabase.com/docs/latest/.</p> <p>This is the option selected for this workshop on my side, by doing this:</p> <ol> <li>I'm not admin on my professional laptop, so I launched an OVHcloud virtual marchine (instance).</li> <li>Go in <code>OVHcloud Control panel / Public Cloud / Instance / new</code>.</li> <li>create a new instance, from Discovery range (the cheapest) with latest Ubuntu (22.10 at the time) and public network (you need a SSH key to complete the installation).</li> <li>Once created, follow official Metabase instructions from their website documentation</li> </ol> <p>If you followed official instrcutions, shortly after installation you can connect to your metabase instance via :3000&gt; and be redirected to the welcome screen."},{"location":"part5metabase/#connect-your-datawarehouse-to-metabase","title":"Connect your datawarehouse to Metabase","text":"<p>Metabase comes with a large choice of official data sources connectors, but also partners and community-based ones. MySQL, PostgreSQL, MongoDB, BigQuery, Snowflake, Apache Spark, Google Analytics, ... but also Hydra, DuckDB and more.</p> <p> List of official connectors: https://www.metabase.com/data_sources/.</p> <p>Once installed, click on <code>Get started</code>, fill the informations and add a PostgreSQL source.</p> <p>For an OVHcloud for PostgreSQL database, fill is at below (be careful, SSL is in <code>require</code> mode):</p> <p> </p> <p>If your connector is correctly configured, you should be able to browse you data through the left menu.</p> <p>Metabase provides a sample database, and one that we called <code>Workshop</code>.</p> <p></p> <p>If you browse this data, you will eventually see all the tables generated via DBT. The ones with raw data, and the ones generated (<code>customers</code> and <code>countries</code>).</p> <p>Metabase built a handy feature called X-ray (the lightning strike), generating automatically some reports about a specific table. Try it with your customers table for example :</p> <p></p>"},{"location":"part5metabase/#build-awesome-dashboards","title":"Build awesome dashboards","text":"<p>When you query you data, Metabase allows you to save your query and his visualisation.  A Dashboard is composed to saved visualisation and filtering options.  You can see a dashboard as virtual collection of previously-made visualizations. </p> <p>So, you start by querying the data, then save the results and finally add them into dashboards.</p>"},{"location":"part5metabase/#query-your-data","title":"Query your data","text":"<p>On the top right menu, click on the <code>+ NEW</code> button and select <code>Question</code>.</p> <p>Pick the <code>/Workshop/customers/</code> data table. It will open a visual interface allowing you to filter, sort, limit your queries, as classic SQL queries. You also have advanced features like SQL join, ...</p> <p>This interface is pretty straightforward, play a bit with the tool. As an example, do a <code>Count of rows</code> by <code>country</code>, then click on the small <code>Preview</code>button on the right.</p> <p></p> <p> on the <code>+ NEW</code> button, you can also create a new SQL query via code editor. You can also create Metabase models, like in DBT. you have the ability to synchronize your DBT models and documentation with Metabase via extra python package such as dbt-metabase.</p>"},{"location":"part5metabase/#add-a-visualization-to-the-dashboard","title":"Add a visualization to the dashboard","text":"<p>Once your query fits your need, click on <code>Visualize</code>. on the left menu, you can modify visualization type and few settings.</p> <p>Example:</p> <p></p> <p>Save your question when you're satisfied ! Metabase will automatically suggest to add this question to a dashboard.</p>"},{"location":"part5metabase/#exercise-3-build-your-first-dashboard","title":"Exercise 3: build your first dashboard","text":"<p>Our sales team discovered your BI platform project. They would love a first dashboard with 4 informations. we don't judge their relevancy here  :</p> <ul> <li>Total of customers we have.</li> <li>Table with top 5 customers information.</li> <li>Bar chart with orders repartition per customers.</li> <li>Cascade chart with revenues and orders per country.</li> </ul> <p>All of that with a dashboard filter, per country.</p> <p>At the end it should look a bit like this:</p> <p></p> <p> If you have some issues with this exercise, you'll find queries details in this repository, as always!</p>"},{"location":"part5metabase/#dashboard-made-easy","title":"Dashboard made easy!","text":"<p>As you can feel it, Metabase is cool. tons of features yet quite simple to install and run. With an open source mindset.</p> <p>This workshop is over, time for a quick conclusion!</p>"},{"location":"part6conclusion/","title":"Conclusion","text":""},{"location":"part6conclusion/#done-you-just-built-a-first-bi-platform","title":"Done! You just built a first BI platform","text":"<p>High five! Thank you for following this workshop.</p> <p>Initial goal was to discover awesome and open source data tools, how they work, and with a common use-case.  I do hope that this mission is completed!</p> <p>Do hesitate to improve this repository directly (pull request), open an issue, or share your thoughts directly.</p> <p>Feel free to contribute (see the Github link on the top bar ?) and give me your feedback via http://twitter.com/bastienovh/ .</p> <p>And don't forget to tell your mom about what you did today... </p>"},{"location":"part6conclusion/#additional-resources","title":"Additional resources","text":"<ul> <li>Curated list of DBT resources: https://github.com/Hiflylabs/awesome-dbt/</li> <li>Official DBT workshop, who inspired this one: https://github.com/dbt-labs/jaffle_shop/</li> <li>Official Metabase learn lessons for much better dashboard: https://www.metabase.com/learn/</li> </ul>"}]}